# Example environment configuration (.env)

##### LLM Provider Selection #####
# True -> use external OpenAI-compatible API, False -> use local Ollama
USE_EXTERNAL_API=False

##### Ollama (local) #####
OLLAMA_URL=http://localhost:11434

##### External API (OpenAI-compatible) #####
# Primary API key (also accepts OPENAI_API_KEY)
EXTERNAL_API_KEY=your-api-key-here
# Optional custom endpoint
EXTERNAL_API_URL=https://api.openai.com/v1/chat/completions
# Azure OpenAI example:
# EXTERNAL_API_URL=https://your-resource.openai.azure.com/openai/deployments/your-deployment/chat/completions?api-version=2024-02-01

##### Model configuration #####
CHAT_MODEL=llama3.1           # none: llama3.1
CHAT_CTX_WINDOW=16000
# HELPER_MODEL=mistral          # none: use CHAT_MODEL
HELPER_CTX_WINDOW=4096
HISTORY_LENGTH=4

##### Embedding & chunking #####
EMB_MODEL=all-MiniLM-L6-v2
MAX_CHARS=1200
OVERLAP=150

##### Retrieval & iteration #####
TOP_K=5
MAX_ITERATIONS=3

##### Cleaning / formatting #####
LINK_MODE=wiki             # wiki | title | url | strip

##### App flags #####
BOT_NAME=Kaito
KEEP_INDEX=False
VERBOSE=False
MODE=1                     # 1 search, 2 ask, 3 teach
CODEBASE_FLAG=False
